{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Learning Overview\n",
    "\n",
    "- A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.\n",
    "\n",
    "- Some basic terms:\n",
    "    - Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "    - Splitting: It is a process of dividing a node into two or more sub-nodes.\n",
    "    - Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "    - Leaf/Terminal Node: Nodes do not split is called Leaf or Terminal node.\n",
    "    - Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "    - Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree.\n",
    "    - Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes whereas sub-nodes are the child of parent node.\n",
    "    ![image from gdcoder](https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-18-at-03.40.41.png)\n",
    "\n",
    "\n",
    "\n",
    "- A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules\n",
    "- Learning from decision trees uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves)\n",
    "- Decision tree learning is the construction of a decision tree from class-labeled training tuples. A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node\n",
    "- Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels.This analysis is classification analysis\n",
    "- Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.This analysis is regression analysis\n",
    "\n",
    "\n",
    "Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items using suitable metrics. Some common metrics used in decision trees algorithms are:\n",
    "\n",
    "\n",
    "**Information gain : (Used by the ID3, C4.5 and C5.0 tree-generation algorithms)**\n",
    "\n",
    "It is based on the concept of entropy and information content from information theory.\n",
    "            \n",
    "Information gain IG(A) is the measure of the difference in entropy from before to after the set S is split on an attribute A. \n",
    "In other words, how much uncertainty in S was reduced after splitting set S on attribute A.\n",
    "\n",
    "![Wikipedia image](https://wikimedia.org/api/rest_v1/media/math/render/svg/b457b0e91764ddd152a362572fc0b6b9a5af3b82)\n",
    "\n",
    "                    - H(s) -Entropy(measure of the amount of uncertainty in the (data) set S) calculated as: \n",
    "              \n",
    "![wikiimg](https://wikimedia.org/api/rest_v1/media/math/render/svg/67d6db8ff7b06b9545b541abc6a8cb3c09cea6b9) \n",
    "\n",
    "                     - T -The subsets created from splitting set S by attribute A \n",
    "                     \n",
    "                     - P(t)-The proportion of the number of elements in t to the number of elements in set S \n",
    "                     - H(t)- Entropy of subset t \n",
    "                     - The attribute with the largest information gain is used to split the set S on this iteration \n",
    "     \n",
    "       \n",
    "**Gini impurity:Used by the CART (classification and regression tree)** \n",
    " \n",
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was           randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "It reaches its minimum (zero) when all cases in the node fall into a single target category.\n",
    "To compute Gini impurity for a set of items with J classes,suppose i =1 to i=j and let Pi be the fraction of items labeled with class i in the set:\n",
    "\n",
    "\n",
    "![wikimedia image ](https://wikimedia.org/api/rest_v1/media/math/render/svg/e3c72a8e705c2fddf2fd552241c579a6c146af7f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**How decision tree(CART) is built in brief steps?**\n",
    "\n",
    "STEP1: Compute the gini index for data-set\n",
    "\n",
    "STEP2: For every attribute/feature:\n",
    " - Calculate gini index for all categorical values \n",
    " - Take average information entropy for the current attribute \n",
    " - Calculate the gini gain\n",
    "       \n",
    "STEP3: Pick the best gini gain attribute.\n",
    "\n",
    "STEP4: Repeat from STEP2 until we get desired tree.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Decision Tree regression:(Regression trees)\n",
    "\n",
    "- Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n",
    "\n",
    "- Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items.\n",
    "\n",
    "- Regression trees use multiple algorithms like CART,MARS to decide to split a node into two or more sub-nodes.\n",
    "\n",
    "- CART:\n",
    "\n",
    "- A node represents a single input variable (X) and a split point on that variable, assuming the variable is numeric. The leaf nodes (also called terminal nodes) of the tree contain an output variable (y) which is used to make a prediction\n",
    "\n",
    "- The representation of the CART model is a binary tree.\n",
    "\n",
    "- Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divide the space called recursive binary splitting. This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function.\n",
    "\n",
    "-  All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function. The cost function that is minimized to choose split points is the mean squared error across all training samples that fall within the rectangle.\n",
    "\n",
    "- Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached\n",
    "\n",
    "\n",
    "**How decision trees regression works?**\n",
    "\n",
    "- Suppose there are two independent variables (x1 and x2) and one dependent variable or target variable(y)\n",
    "\n",
    "- A variable is choosen that best splits the data.Decision trees regression normally use mean squared error (MSE) to decide to split a node in two or more sub-nodes.The algorithm first will pick a value, and split the data into two subset. For each subset, it will calculate the MSE separately. The tree chooses the value with results in smallest MSE value.A way to find the best split which is to try every variable and to try every possible value of that variable and see which variable and which value gives us a split with the best score.Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached.This split will make different rectangular partitions in dataset as shown in figure \n",
    "\n",
    "- For each partition average target value of the points in calculated and taken as leaf node or terminal node. This node is actually used to provides the value for the regression model.For new observation of data points ie x1 and x2 , this decision trees actually works top down from the tree and make decision choosing suitable split to finally reach terminal node which gives a numeric value that is the predicted regression output.\n",
    "\n",
    "![](https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-17-at-00.09.26.png)\n",
    "\n",
    "\n",
    "\n",
    "Advantages of Decision trees:\n",
    "\n",
    "- Are simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\n",
    "- Have value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.\n",
    "- Help determine worst, best and expected values for different scenarios.\n",
    "- Use a white box model. If a given result is provided by a model.\n",
    "- Can be combined with other decision techniques.\n",
    "\n",
    "\n",
    "Disadvantage of Decision trees:\n",
    "- They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.\n",
    "- They are often relatively inaccurate. Many other predictors perform better with similar data. This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.\n",
    "- For data including categorical variables with different number of levels, information gain in decision trees is biased in favor of those attributes with more levels.\n",
    "- Calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART(Classification and Regression Trees)\n",
    "\n",
    "-  Classification And Regression Tree (CART) analysis is term used to refer to both of the Classification and Regression analysis of decision Trees using Gini index as metrice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of CART using libraries\n",
    "\n",
    "Libraries Doc Links:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.colors.ListedColormap.html\n",
    "\n",
    "https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.scatter.html\n",
    "\n",
    "https://matplotlib.org/3.1.1/api/colors_api.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62  6]\n",
      " [ 3 29]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decision Tree Classification\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as col\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/shivang98/Social-Network-ads-Boost/master/Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "A quick look at some notable parameters on DecisionTreeClassifier class :\n",
    "random_state:\n",
    "If int, random_state is the seed used by the random number generator; \n",
    "If RandomState instance, random_state is the random number generator; \n",
    "If None, the random number generator is the RandomState instance used by np.random.\n",
    "if random_state is set to some then same seed is used ie same result in each time run .For instance when used in train_test split same train test will be done in multiple runs\n",
    "max_leaf_nodes:\n",
    "Used to control the leaf nodes in trees (ie we can tune  overfitting ,underfitting using this values.)\n",
    "If None then unlimited number of leaf nodes.\n",
    "others see on doc \n",
    "\"\"\"\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = col.to_rgba_array(ListedColormap(('red', 'green'))(i)), label = j)\n",
    "plt.title('Decision Tree Classification (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = col.to_rgba_array(ListedColormap(('red', 'green'))(i)), label = j)\n",
    "plt.title('Decision Tree Classification (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150000.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/lucko515/regression-python/master/Salaries%20based%20on%20Positions%20-%20dataset/Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "\"\"\"from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\"\"\"\n",
    "\n",
    "# Feature Scaling\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\"\"\"\n",
    "\n",
    "# Fitting Decision Tree Regression to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y) #ie fitting all values\n",
    "\n",
    "#Note:no test performed here ie. Model is not evaluated\n",
    "\n",
    "# Predicting a new result\n",
    "y_pred = regressor.predict([[6.5]])\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Visualising the Decision Tree Regression results (higher resolution)\n",
    "X_grid = np.arange(min(X), max(X), 0.01)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Truth or Bluff (Decision Tree Regression)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
