{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "#### Well-posed & ill-posed problems\n",
    " According to Jacques Hadamard mathematical models of physical phenomena or well posed problems should have the properties that:\n",
    " - a solution exists\n",
    " - the solution is unique\n",
    " - the solution's behavior changes continuously with the initial conditions\n",
    " \n",
    "Examples of well-posed problems include the Dirichlet problem for Laplace's equation, and the heat equation with specified initial conditions.\n",
    "\n",
    "Problems that are not well-posed in the sense of Hadamard are termed ill-posed.\n",
    "\n",
    "Inverse problems are often ill-posed. For example, the inverse heat equation, deducing a previous distribution of temperature from final data, is not well-posed in that the solution is highly sensitive to changes in the final data.\n",
    "\n",
    "If the problem is well-posed, then it stands a good chance of solution on a computer using a stable algorithm. If it is not well-posed, it needs to be re-formulated for numerical treatment. Typically this involves including additional assumptions, such as smoothness of solution. This process is known as **regularization**.So it is the process of adding information in order to solve an ill-posed problem or to prevent overfitting\n",
    "\n",
    "For any machine learning problem, essentially, you can break your data points into two components â€” pattern + stochastic noise. Now the goal of machine learning is to model the pattern and ignore the noise. Anytime an algorithm is trying to fit the noise in addition to the pattern, it is overfitting.\n",
    "\n",
    "Collinearity is a linear association between two explanatory variables. Two variables are perfectly collinear if there is an exact linear relationship between them.Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related\n",
    "\n",
    "Ridge regression  is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters\n",
    "\n",
    "https://www.quora.com/What-is-regularization-in-machine-learning..\n",
    "\n",
    "https://towardsdatascience.com/regularization-an-important-concept-in-machine-learning-5891628907ea...\n",
    "\n",
    "& other useful resources..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
