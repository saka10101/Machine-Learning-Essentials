{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD(Stochastic Gradient Descent) Classification & Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "- Gradient Descent is an optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function(objective function).\n",
    "\n",
    "- In machine learning, we use mainly gradient descent to update the parameters of our model. Parameters may for instance refer to coefficients in Linear Regression and weights in neural networks and so on.\n",
    "\n",
    "##### How Gradient Descent works?\n",
    "\n",
    " In simple steps:\n",
    " ![Credits: Simplilearn](https://www.simplilearn.com/ice9/free_resources_article_thumb/gradient-descent-learning-graph-machine-learning.JPG)\n",
    "\n",
    "- Step 1: Initialize the parameters or coefficients with some random values and calculate the value of the cost function\n",
    "\n",
    "- Step 2: Calculate the slope of the cost function with respect to each parameter.In other words, compute the gradient function of the cost function. Our concern here is to find change in cost function when the parameters are changed by a very small value from their original randomly initialized value.\n",
    "\n",
    "- Step 3: Update the gradient function by plugging in the parameter values\n",
    "\n",
    "- Step 4: Adjust the parameters with the gradients to reach the optimal values where cost finction is minimized\n",
    "                - Calculate the step sizes for each parameters [step size = gradient * learning rate]\n",
    "                - Use the new parameters[new params = old params -step size] for prediction and to calculate the new cost function value\n",
    "\n",
    "- Step 5: Repeat steps 3 and 4 until gradient is almost 0 or till the further adjustments to weights donâ€™t significantly reduce the Error\n",
    "\n",
    "\n",
    "##### Note on Learning rate:\n",
    "\n",
    "The learning rate mentioned above is a flexible parameter which heavily influences the convergence of the algorithm.\n",
    "- Larger learning rates makes the algorithm take huge steps down the slope and it might jump across the minimum point thereby missing it.\n",
    "- Smaller learning rates makes the algorithm converge to minimum point but very slowly \n",
    "![Credits:Simplilearn](https://www.simplilearn.com/ice9/free_resources_article_thumb/gradient-learning-rate-machine-learning.JPG)\n",
    "\n",
    "\n",
    "From above steps Gradient Descent algorithm usually finds a local minimum of a differentiable function(cost function)by taking steps proportional to the negative of the gradient of the function at the current point.\n",
    "\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "- Gradient descent can be slow to run on very large datasets.Because one iteration of the gradient descent algorithm requires a prediction for each instance in the training dataset, it can take a long time when you have many millions of instances.Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima is reached\n",
    "\n",
    "- In situations when you have large amounts of data, you can use a variation of gradient descent called stochastic gradient descent(SGD).\n",
    "\n",
    "- In this variation, the gradient descent procedure described above is run but the update to the coefficients or parameters is performed for each training instance, rather than at the end of the batch of instances. SGD uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration.\n",
    "\n",
    "- Stochastic Gradient Descent replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data).Especially in big data applications this reduces the computational burden, achieving faster iterations in trade for a slightly lower convergence rate\n",
    "\n",
    "\n",
    "##### How SGD works in brief?\n",
    "\n",
    "Consider the problem of minimizing an objective function(Cost function or loss function) that has the form of a sum: \n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7f38e0bdfea090cfd651222e7db9806dce6164cd)\n",
    "\n",
    "where,\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/25b1170e62c103ff59c79ea424f1c409c4742225) gives the value of the loss function at i-th observation in the data set(used for training)\n",
    "\n",
    "\n",
    "Stochastic gradient works simply by following steps:\n",
    "\n",
    "- Step1: Firts choose \n",
    "    - initial vector of parameters:------->>\n",
    "     ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/88b1e0c8e1be5ebe69d18a8010676fa42d7961e6)\n",
    "\n",
    "    - learning rate:--------->>\n",
    "    ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64)\n",
    "\n",
    "- Step2: Randomly shuffle examples in the training set\n",
    "\n",
    "- Step3: Update or adjust the parameters\n",
    "\n",
    "        - For every suffled examples in training sets i=1,2,...,n adjust the each parameter as following:\n",
    "     ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4dec506d9a4c822ef0a4519d823ccd80ad8b79bc)\n",
    "    \n",
    "- Step4: Repeat Step 2 to 3 until an approximate minimum is obtained \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Stochastic gradient descent method of optimization that can be used to train models such as linear support vector machines (SVMs) and logistic regression models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Stochastic Gradient Descent(SGD) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
